import os
import torch
import numpy as np
import cv2
from PIL import Image
from tqdm import tqdm
import argparse
import copy
from omegaconf import OmegaConf
from einops import rearrange
from pytorch_lightning import seed_everything
from contextlib import nullcontext

from ldm.util import instantiate_from_config
from ldm.models.diffusion.plms import PLMSSampler

from ldm.models.diffusion.ddim import DDIMSampler

import torchvision.transforms as transforms
import torch.nn.functional as F
import time
import pickle

'''
python run_styleid_class.py 
        '''
class StyleID:
    def __init__(
        self,
        model_config_path="models/ldm/stable-diffusion-v1/v1-inference.yaml",
        model_ckpt_path="/Users/ran/Documents/Datasets/HE_Patch/sd-v1-4.ckpt",
        precomputed_dir="precomputed_feats",
        output_dir="output",
        device="cpu",
        seed=22,
        gamma=0.75,
        temperature=1.5,
        attn_layers="6,7,8,9,10,11",
        precision="full",
    ):
        self.model_config_path = model_config_path
        self.model_ckpt_path = model_ckpt_path
        self.precomputed_dir = precomputed_dir
        self.output_dir = output_dir

        # åˆ›å»ºè¾“å‡ºå–é¢„è®¡ç®—ç‰¹å¾ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        if precomputed_dir:
            os.makedirs(precomputed_dir, exist_ok=True)
            
        # if device is None:
        #     self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # else:   
        #     self.device = torch.device(device)
        if device is None:
            if force_cpu:
                self.device = torch.device("cpu")
                print("å¼ºåˆ¶ä½¿ç”¨ CPU")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                print("ä½¿ç”¨ CUDA åŠ é€Ÿ")
            elif hasattr(torch, 'has_mps') and torch.has_mps and torch.backends.mps.is_available():
                # Mac ä¸Šçš„ MPS è®¾å¤‡ä¼šå¯¼è‡´é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨ CPU
                print("æ£€æµ‹åˆ° MPS è®¾å¤‡ï¼Œä½†å› å…¼å®¹æ€§é—®é¢˜å¼ºåˆ¶ä½¿ç”¨ CPU")
                self.device = torch.device("cpu")
            else:
                self.device = torch.device("cpu")
                print("ä½¿ç”¨ CPU")
        else:
            if device == "mps" or (isinstance(device, torch.device) and device.type == "mps"):
                print("MPS è®¾å¤‡å¯èƒ½å¯¼è‡´å…¼å®¹æ€§é—®é¢˜ï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU")
                self.device = torch.device("cpu")
            else:
                self.device = torch.device(device)
            
        # åœ¨ CPU ä¸Šè®¾ç½®ç²¾åº¦ä¸º "full"
        if self.device.type == "cpu":
            self.precision = "full"
            print("åœ¨ CPU ä¸Šä½¿ç”¨ full ç²¾åº¦")

        seed_everything(seed)

        self.seed = seed
        self.gamma = gamma
        self.temperature = temperature
        self.attn_layers = list(map(int, attn_layers.split(',')))
        self.precision = precision

        self.feat_maps = []
        # åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–é‡‡æ ·å™¨ 
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_ckpt_path}")
        self.model = self._load_model(model_ckpt_path, model_config_path)
        
        # åˆå§‹åŒ– DDIM é‡‡æ ·å™¨
        print("æ­£åœ¨åˆå§‹åŒ– DDIM é‡‡æ ·å™¨")
        self.sampler = DDIMSampler(self.model)
        
        # è·å– UNet æ¨¡å‹ä»¥ä¾¿æå–ç‰¹å¾
        self.unet = self.model.model.diffusion_model
        
        print("æ¨¡å‹åŠ è½½å®Œæˆ")

    def _load_model(self, model_path, config_path):
        print(f"Loading model from {model_path} with config {config_path}")
        config = OmegaConf.load(config_path)
        model = instantiate_from_config(config.model).to(self.device)

        # åŠ è½½æƒé‡
        state_dict = torch.load(model_path, map_location="cpu", weights_only=False)["state_dict"]
        model.load_state_dict(state_dict, strict=False)

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ âœ… å…³é”®ä¸€æ­¥
        # evalé‡è¦ï¼Œæ˜¯pytorchä¸­ç”¨äºå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼çš„æ–¹æ³•
        '''
        âœ¨ 1. Dropout å±‚
        åœ¨è®­ç»ƒæ—¶ï¼šDropout ä¼šéšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼ˆæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼‰
        åœ¨è¯„ä¼°æ—¶ï¼ˆ.eval()ï¼‰ï¼šDropout è¢«å…³é—­ï¼ˆä¸ä¼šä¸¢å¼ƒç¥ç»å…ƒï¼‰
        ğŸ“Š 2. BatchNorm å±‚
        åœ¨è®­ç»ƒæ—¶ï¼šä½¿ç”¨å½“å‰ batch çš„å‡å€¼å’Œæ–¹å·®
        åœ¨è¯„ä¼°æ—¶ï¼šä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å…¨å±€å‡å€¼å’Œæ–¹å·®
                '''
        model.eval()

        return model
    
    def _setup_ddim_sampler(self, ddim_steps, ddim_eta=0.0):
        """è®¾ç½® DDIM é‡‡æ ·å™¨å‚æ•°"""
        self.sampler.make_schedule(
            ddim_num_steps=ddim_steps, 
            ddim_eta=ddim_eta, 
            verbose=False
        )
        
        # è·å–æ—¶é—´æ­¥é•¿
        self.time_range = np.flip(self.sampler.ddim_timesteps)
        
        # åˆ›å»ºæ—¶é—´æ­¥å’Œç´¢å¼•çš„æ˜ å°„
        self.idx_time_dict = {}
        self.time_idx_dict = {}
        for i, t in enumerate(self.time_range):
            self.idx_time_dict[t] = i
            self.time_idx_dict[i] = t

    def _save_feature_map(self, feature_map, filename, time):
        # ä¿å­˜å•ä¸ªç‰¹å¾å›¾
        # cur_idx = self.idx_time_dict[time]
        # self.feat_maps[cur_idx] = feat_maps
        cur_idx = self.idx_time_dict[time]
        self.feat_maps[cur_idx][f"{filename}"] = feature_map

    def _save_feature_maps(self, blocks, i, feature_type="input_block"):
        # ä¿å­˜ç‰¹å®šå—çš„
        block_idx = 0
        for block_idx, block in enumerate(blocks):
            if block_idx > 1 and "SpatialTransformer" in str(type(block[1])):
                if  block_idx in self.attn_layers:
                    q = block[1].transformer_blocks[0].attn1.q
                    k = block[1].transformer_blocks[0].attn1.k
                    v = block[1].transformer_blocks[0].attn1.v
                    self._save_feature_map(q, f"{feature_type}_{block_idx}_self_attn_q", i)
                    self._save_feature_map(k, f"{feature_type}_{block_idx}_self_attn_k", i)
                    self._save_feature_map(v, f"{feature_type}_{block_idx}_self_attn_v", i)
            block_idx += 1


    # å›æ‰å‡½æ•°äº‹å…ˆå®šä¹‰å¥½ï¼Œä½†ä¸ä¼šç«‹åˆ»æ‰§è¡Œçš„ï¼Œç­‰åˆ°æœªæ¥æŸä¸ªæ—¶é—´è§¦å‘çš„å‡½æ•°
    # å¤–å±‚ callback â†’ è°ƒç”¨ save_feature_maps â†’ å¤šæ¬¡è°ƒç”¨ save_feature_map
    # ä¿å­˜ä¸­é—´ç‰¹å¾å›¾ï¼Œå¯è§†åŒ–ï¼ŒæŸ¥çœ‹å›¾åƒæ˜¯æ€ä¹ˆä¸€æ­¥æ­¥ç”Ÿæˆçš„
    def _save_feature_maps_callback(self, i):
        """ä¿å­˜ç‰¹å¾å›¾çš„å›è°ƒå‡½æ•°"""
        # åŸä»£ç è°ƒç”¨äº†ä¸å­˜åœ¨çš„æ–¹æ³• _save_feat_map
        # ä¿®æ”¹ä¸ºè°ƒç”¨å·²æœ‰çš„ _save_feature_maps æ–¹æ³•
        self._save_feature_maps(self.unet.output_blocks, i, "output_block")

    def _ddim_sampler_callback(self, pred_x0, xt, i):
        """DDIM é‡‡æ ·å™¨å›è°ƒï¼Œç”¨äºä¿å­˜ç‰¹å¾å›¾"""
        self._save_feature_maps_callback(i)
        self._save_feature_map(xt, 'z_enc', i)

    def _feat_merge(self, cnt_feat, sty_feat, start_step=0):
    # è®¡ç®—æƒé‡
        result_feat_maps = [{'config':{'gamma':self.gamma, 'T':self.temperature, 'timestep':_,}} for _ in range(50)]

        for i in range(len(result_feat_maps)):
            if i < (50 - start_step):
                continue
            # ä¿®å¤å˜é‡ä½¿ç”¨é”™è¯¯ - åŸä»£ç åœ¨å¾ªç¯å†…é‡å¤ä½¿ç”¨ cnt_feat å’Œ sty_feat
            cnt_feat_i = cnt_feat[i]
            sty_feat_i = sty_feat[i]
            ori_keys = sty_feat_i.keys()

            for ori_key in ori_keys:
                if ori_key[-1] == 'q':
                    result_feat_maps[i][ori_key] = cnt_feat_i[ori_key]
                if ori_key[-1] == 'k' or ori_key[-1] == 'v':
                    result_feat_maps[i][ori_key] = sty_feat_i[ori_key]
        return result_feat_maps

    def _load_img(self, path, h=512, w=512):
        """åŠ è½½å¹¶å›¾åƒ"""
        image = Image.open(path).convert("RGB")
        x, y = image.size
        print(f"Loaded input image of size ({x}, {y}) from {path}")

        image = image.resize((w, h), resample=Image.Resampling.LANCZOS)
        image = np.array(image).astype(np.float32) / 255.0
        image = image[None].transpose(0, 3, 1, 2)
        image = torch.from_numpy(image)
        return 2.*image - 1.
    
    def _adain(self, cnt_feat, sty_feat):
        """è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–
        æŠŠ cnt_feat çš„ç‰¹å¾ æ ‡å‡†åŒ– åï¼Œé‡æ–°èµ‹äºˆ sty_feat çš„å‡å€¼å’Œæ–¹å·®ï¼Œæœ€ç»ˆè¿”å›ä¸€ä¸ªæ–°çš„ feature mapï¼Œé£æ ¼æ¥è‡ª style å›¾ï¼Œç»“æ„æ¥è‡ª content å›¾ã€‚
        """
        cnt_mean = cnt_feat.mean(dim=[0, 2, 3], keepdim=True)
        cnt_std = cnt_feat.std(dim=[0, 2, 3], keepdim=True)
        sty_mean = sty_feat.mean(dim=[0, 2, 3], keepdim=True)
        sty_std = sty_feat.std(dim=[0, 2, 3], keepdim=True)
        # å– æ¯ä¸ªé€šé“ï¼ˆdim=0, 2, 3 â†’ batch, height, widthï¼‰ä¸Šçš„ mean å’Œ std
        output = ((cnt_feat-cnt_mean)/cnt_std)*sty_std + sty_mean
        # å†…å®¹å›¾ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè¿™ä¸€æ­¥ç›¸å½“äºå»æ‰å†…å®¹å›¾çš„æœ¬èº«ç‰¹å¾ï¼Œç”¨é£æ ¼å›¾çš„ç»Ÿè®¡ä¿¡æ¯é‡å»ºåˆ†å¸ƒï¼‰
        # æ ¸å¿ƒæ„ä¹‰ï¼šä¿ç•™å†…å®¹å›¾çš„å½¢çŠ¶ï¼Œä½†è®©å®ƒçš„ç‰¹å¾åˆ†å¸ƒçœ‹èµ·æ¥åƒé£æ ¼å›¾

        return output  
    
    def transfer_style(
            self,
            content_img_path,
            style_img_path,
            output_name = None,
            ddim_steps=50,
            ddim_eta=0.0,
            ddim_scale=1.0,
            start_step=49,
            use_adain=True,
            use_attn_injection=True,
            h=512,
            w=512,
    ):
        self._setup_ddim_sampler(ddim_steps, ddim_eta)

        if output_name is None:
            output_name = os.path.basename(content_img_path).split('.')[0]
            style_name = os.path.basename(style_img_path).split('.')[0]
            output_name = f"{output_name}_stylized_{style_name}.png"

        output_path = os.path.join(self.output_dir, output_name)

        self.feat_maps = [{
            'config':{
                'gamma':self.gamma,
                'T':self.temperature,
            }
        }for _ in range(50)]

        uc = self.model.get_learned_conditioning([""])

        shape = (4, h // 8, w // 8)   # latent spaceä¸­çš„å¤§å°



        # åŠ è½½é£æ ¼å›¾åƒå¹¶è·å–ç‰¹å¾
        print(f"Loading style image from {style_img_path}")
        sty_feat_name = os.path.join(
            self.precomputed_dir,
            os.path.basename(style_img_path).split('.')[0] + "_sty.pkl"
        )
        
        if self.precomputed_dir and os.path.exists(sty_feat_name):
            with open(sty_feat_name, "rb") as h:
                sty_feat = pickle.load(h)
                sty_z_enc = torch.clone(sty_feat[0]['z_enc'])
        else:
            init_sty = self._load_img(style_img_path, 512, 512).to(self.device)
            init_sty = self.model.get_first_stage_encoding(
                self.model.encode_first_stage(init_sty) # é€å…¥vaeç¼–ç å™¨ï¼Œå¾—åˆ°ç¼–ç åè¡¨ç¤ºfeature map
            )
            sty_z_enc, _ = self.sampler.encode_ddim(
                init_sty.clone(),
                num_steps=ddim_steps,
                unconditional_conditioning = uc,
                end_step = self.time_idx_dict[ddim_steps-1-start_step],
                callback_ddim_timesteps = ddim_steps,
                img_callback = self._ddim_sampler_callback,
            )
            sty_feat = copy.deepcopy(self.feat_maps)
            sty_z_enc = self.feat_maps[0]['z_enc']
            
            # ä¿å­˜é£æ ¼ç‰¹å¾
            if self.precomputed_dir:
                with open(sty_feat_name, 'wb') as h:
                    pickle.dump(sty_feat, h)
        
        # åŠ è½½å†…å®¹å›¾åƒå¹¶è·å–ç‰¹å¾
        print(f"å¤„ç†å†…å®¹å›¾åƒ: {content_img_path}")
        cnt_feat_name = os.path.join(
            self.precomputed_dir, 
            os.path.basename(content_img_path).split('.')[0] + '_cnt.pkl'
        )

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„è®¡ç®—çš„å†…å®¹ç‰¹å¾
        if self.precomputed_dir and os.path.isfile(cnt_feat_name):
            print(f"åŠ è½½é¢„è®¡ç®—å†…å®¹ç‰¹å¾: {cnt_feat_name}")
            with open(cnt_feat_name, 'rb') as h:
                cnt_feat = pickle.load(h)
                cnt_z_enc = torch.clone(cnt_feat[0]['z_enc'])
        else:
            # åŠ è½½å¹¶ç¼–ç å†…å®¹å›¾åƒ
            init_cnt = self._load_img(content_img_path, 512, 512).to(self.device)
            init_cnt = self.model.get_first_stage_encoding(
                self.model.encode_first_stage(init_cnt)
            )
            
            # å†…å®¹å›¾åƒ DDIM åè½¬
            cnt_z_enc, _ = self.sampler.encode_ddim(
                init_cnt.clone(), 
                num_steps=ddim_steps, 
                unconditional_conditioning=uc,
                end_step=self.time_idx_dict[ddim_steps-1-start_step],
                callback_ddim_timesteps=ddim_steps,
                img_callback=self._ddim_sampler_callback
            )
            cnt_feat = copy.deepcopy(self.feat_maps)
            cnt_z_enc = self.feat_maps[0]['z_enc']
            
            # ä¿å­˜å†…å®¹ç‰¹å¾
            # if self.precomputed_dir:
            #     with open(cnt_feat_name, 'wb') as h:
            #         pickle.dump(cnt_feat, h)
        
        # if self.precision == "autocast" and torch.cuda.is_available():
        #     precision_scope = lambda: autocast("cuda")
        # else:
        precision_scope = nullcontext
        
        with torch.no_grad():
            with precision_scope():
                with self.model.ema_scope():
                    # AdaIN å¤„ç†æ½œåœ¨ç¼–ç 
                    if not use_adain:
                        adain_z_enc = cnt_z_enc
                    else:
                        adain_z_enc = self._adain(cnt_z_enc, sty_z_enc)
                    
                    # åˆå¹¶ç‰¹å¾
                    # merged_feat_maps = self._feat_merge(
                    #     cnt_feat, sty_feat, start_step=start_step
                    # )
                    self.feat_maps = self._feat_merge(cnt_feat, sty_feat, start_step=start_step)
                    
                    # å¦‚æœä¸ä½¿ç”¨æ³¨æ„åŠ›æ³¨å…¥åˆ™è®¾ç½®ä¸º None
                    if not use_attn_injection:
                        self.feat_maps = None
                    
                    # é‡‡æ ·ç”Ÿæˆç»“æœ
                    samples_ddim, _ = self.sampler.sample(
                        S=ddim_steps,
                        batch_size=1,
                        shape=shape,
                        verbose=False,
                        unconditional_conditioning=uc,
                        eta=ddim_eta,
                        x_T=adain_z_enc,
                        injected_features=self.feat_maps,
                        start_step=start_step,
                        
                    )
                    
     # è§£ç ç”Ÿæˆå›¾åƒ
                    x_samples_ddim = self.model.decode_first_stage(samples_ddim)
                    x_samples_ddim = torch.clamp(
                        (x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0
                    )
                    x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()
                    x_image_torch = torch.from_numpy(x_samples_ddim).permute(0, 3, 1, 2)
                    x_sample = 255. * rearrange(x_image_torch[0].cpu().numpy(), 'c h w -> h w c')
                    img = Image.fromarray(x_sample.astype(np.uint8))
                    
                    # ä¿å­˜ç»“æœå›¾åƒ
                    img.save(output_path)

        print(f"é£æ ¼è¿ç§»å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_path}")
        return output_path
    
    def batch_transfer(
        self, 
        content_dir, 
        style_dir, 
        output_dir=None, 
        **kwargs
    ):
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„å›¾åƒ
        
        å‚æ•°:
            content_dir: å†…å®¹å›¾ç‰‡ç›®å½•
            style_dir: é£æ ¼å›¾ç‰‡ç›®å½•
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºå®ä¾‹åŒ–æ—¶è®¾ç½®çš„ç›®å½•
            **kwargs: ä¼ é€’ç»™ transfer_style çš„å‚æ•°
            
        è¿”å›:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if output_dir:
            old_output_dir = self.output_dir
            self.output_dir = output_dir
            os.makedirs(output_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰å†…å®¹å’Œé£æ ¼å›¾ç‰‡
        content_imgs = [f for f in os.listdir(content_dir) 
                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        style_imgs = [f for f in os.listdir(style_dir) 
                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        # æ’åºä»¥ä¿æŒä¸€è‡´æ€§
        content_imgs.sort()
        style_imgs.sort()
        results = []
        total = len(content_imgs) * len(style_imgs)
        
        # æ‰¹é‡å¤„ç†
        with tqdm(total=total, desc="æ‰¹å¤„ç†é£æ ¼è¿ç§»") as pbar:
            print("1")
            for content_img in content_imgs:
                for style_img in style_imgs:
                    content_path = os.path.join(content_dir, content_img)
                    style_path = os.path.join(style_dir, style_img)
                    print("2")
                    
                    result_path = self.transfer_style(
                        content_path, style_path, **kwargs
                    )
                    results.append(result_path)
                    print(f"3")
                    # except Exception as e:
                    #     print(f"4")
                    #     print(f"å¤„ç† {content_img} å’Œ {style_img} æ—¶å‡ºé”™: {e}")
                    
                    pbar.update(1)
        print("5")
        # æ¢å¤åŸå§‹è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = old_output_dir
            
        return results
    

# å‘½ä»¤è¡Œæ¥å£
def main():
    parser = argparse.ArgumentParser(description="StyleID é£æ ¼è¿ç§»")
    parser.add_argument('--cnt', type=str, default='./data/cnt', help='å†…å®¹å›¾åƒç›®å½•æˆ–è·¯å¾„')
    parser.add_argument('--sty', type=str, default='./data/sty', help='é£æ ¼å›¾åƒç›®å½•æˆ–è·¯å¾„')
    parser.add_argument('--output_path', type=str, default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--precomputed', type=str, default='./precomputed_feats', help='é¢„è®¡ç®—ç‰¹å¾å­˜å‚¨ç›®å½•')
    parser.add_argument('--model_config', type=str, default='models/ldm/stable-diffusion-v1/v1-inference.yaml', help='æ¨¡å‹é…ç½®')
    parser.add_argument('--ckpt', type=str, default='/Users/ran/Documents/Datasets/HE_Patch/sd-v1-4.ckpt', help='æ¨¡å‹æƒé‡')
    parser.add_argument('--ddim_steps', type=int, default=50, help='DDIM æ­¥æ•°')
    parser.add_argument('--ddim_eta', type=float, default=0.0, help='DDIM eta')
    parser.add_argument('--start_step', type=int, default=49, help='å¼€å§‹æ­¥éª¤')
    parser.add_argument('--gamma', type=float, default=0.75, help='æŸ¥è¯¢ä¿ç•™è¶…å‚æ•°')
    parser.add_argument('--T', type=float, default=1.5, help='æ³¨æ„åŠ›æ¸©åº¦è¶…å‚æ•°')
    parser.add_argument("--attn_layer", type=str, default='6,7,8,9,10,11', help='æ³¨å…¥ç‰¹å¾å±‚')
    parser.add_argument('--precision', type=str, default='full', help='ç²¾åº¦: full æˆ– autocast')
    parser.add_argument("--without_init_adain", action='store_true', help='ç¦ç”¨ AdaIN')
    parser.add_argument("--without_attn_injection", action='store_true', help='ç¦ç”¨æ³¨æ„åŠ›æ³¨å…¥')
    parser.add_argument('--seed', type=int, default=22, help='éšæœºç§å­')
    
    opt = parser.parse_args()
    
    # åˆ›å»º StyleID æ¨¡å‹
    model = StyleID(
        model_config_path=opt.model_config,
        model_ckpt_path=opt.ckpt,
        precomputed_dir=opt.precomputed,
        output_dir=opt.output_path,
        seed=opt.seed,
        gamma=opt.gamma,
        temperature=opt.T,
        attn_layers=opt.attn_layer,
        precision=opt.precision
    )
 # æ£€æŸ¥è¾“å…¥æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    if os.path.isdir(opt.cnt) and os.path.isdir(opt.sty):
        # æ‰¹é‡å¤„ç†
        print(f"æ‰¹é‡å¤„ç†å†…å®¹ç›®å½•: {opt.cnt} å’Œé£æ ¼ç›®å½•: {opt.sty}")
        model.batch_transfer(
            content_dir=opt.cnt,
            style_dir=opt.sty,
            ddim_steps=opt.ddim_steps,
            ddim_eta=opt.ddim_eta,
            start_step=opt.start_step,
            use_adain=not opt.without_init_adain,
            use_attn_injection=not opt.without_attn_injection
        )
    else:
        # å•å›¾å¤„ç†
        print(f"å¤„ç†å•å¼ å›¾åƒ: {opt.cnt} å’Œ {opt.sty}")
        model.transfer_style(
            content_img_path=opt.cnt,
            style_img_path=opt.sty,
            ddim_steps=opt.ddim_steps,
            ddim_eta=opt.ddim_eta,
            start_step=opt.start_step,
            use_adain=not opt.without_init_adain,
            use_attn_injection=not opt.without_attn_injection
        )
    
    
if __name__ == "__main__":
    main()